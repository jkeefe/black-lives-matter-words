{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mayoral USE searcher for NICAR",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Document Search with Sentence Encoder\n",
        "\n",
        "Originally by by Jeremy Merrill, Quartz.\n",
        "\n",
        "for NICAR 2020.\n",
        "\n",
        "Imagine getting a huge pile of documents. You know that there's interesting stuff in the pile, but you don't know what, exactly. \n",
        "\n",
        "Let's search that pile.\n",
        "\n",
        "Github repo: https://github.com/Quartz/aistudio-searching-data-dumps-with-use / https://github.com/Quartz/aistudio-workshops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63Pd3nJnTl-i"
      },
      "source": [
        "**IMPORTANT** Note: Please select \"**Python 3**\" _and_ \"**GPU**\" in the ***Runtime->Change Runtime type*** dropdown menu above _before_ running this notebook for faster execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This is an interactive demo. You can run all the code necessary to search a pile of documents right here. (A medium-sized, since we don't have all day.)\n",
        "\n",
        "We're using two neat pieces of technology called the *Universal Sentence Encoder* and *Annoy*.\n",
        "\n",
        "- the *Universal Sentence Encoder* is a pre-trained machine-learning model that sorta understands human language. If you feed in a sentence, it comes out with 512 numbers that represent the approximate meaning of that sentence. What's really cool is that if you feed in a second sentence that means about the same thing, that second sentence's numbers will be very close to those of the first sentence.\n",
        "- *Annoy* is a library that makes it really easy to find points in vector space that are close to each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrZqHVFBGfW"
      },
      "source": [
        "What's \"vector space\"? Imagine dot plot with an x-axis and a y-axis. That's two-dimensional vector space.\n",
        "\n",
        "This is three-dimensional vector space. Three axes: x, y, z.\n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-02-28%20at%205.43.59%20PM.png)\n",
        "\n",
        "Now imagine 512 axes. That's what we're dealing with here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZPVY12mDb0P"
      },
      "source": [
        "## Okay, let's get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d6d4e1e8-cbfb-4816-a7f9-81ee14498254"
      },
      "source": [
        "#@title Setup Environment\n",
        "#latest Tensorflow that supports sentencepiece is 1.14\n",
        "!pip uninstall --quiet --yes tensorflow\n",
        "!pip install --quiet tensorflow-gpu==1.14\n",
        "!pip install --quiet tensorflow==1.14\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet bokeh\n",
        "!pip install --quiet tf-sentencepiece\n",
        "!pip install --quiet annoy\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet w3lib\n",
        "!pip install --quiet syntok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 377.0MB 44kB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 48.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 60.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 109.2MB 28kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 3.4MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FhAXYYn-ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "44c68911-0e68-4c9f-a836-9e40409b32d3"
      },
      "source": [
        "!pip install tf-sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/a5/16123d662ebeb087552c39c895e9ec6239fb828e236d95fdf67b20907b27/tf_sentencepiece-0.1.90-py2.py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tf-sentencepiece\n",
            "Successfully installed tf-sentencepiece-0.1.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70822a1f-9876-4ef6-bcb3-3f339782b4ea"
      },
      "source": [
        "#@title Setup common imports and functions\n",
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "mkmF3w8WGLcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d108c821-fe20-4810-8af3-390318c9e8c0"
      },
      "source": [
        "#@title get the machine learning stuff set up. (boilerplate!)\n",
        "# this version of the Universal Sentence Encoder only \"speaks\" English\n",
        "# but there's another version you can switch in that supports 16 different languages!\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
        "\n",
        "# boilerplate, getting started with Tensorflow.\n",
        "# (how to use Tensorflow is way outside the scope of this class)\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  multiling_embed = hub.Module(module_url)\n",
        "  embedded_text = multiling_embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XjNsxF2b7ZU",
        "colab": {}
      },
      "source": [
        "# @ get the data.\n",
        "# let's get our data!\n",
        "# it's a JSONL file, which is a file with one page, as its own JSON document, per line.\n",
        "!wget --quiet -nc -O docs.jsonl https://www.dropbox.com/s/watyjmstbsorsh4/docs.jsonl"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eExMTHZNGn86"
      },
      "source": [
        "## What the heck is JSONL?\n",
        "\n",
        "Don't worry too much about it. It looks like this, but there's nothing special to it, it's just a way to get the content of the pages in [these emails](https://github.com/Quartz/aistudio-doc2vec-for-investigative-journalism/blob/master/2018.05.24_BerlinRosen_Responsive_Records.pdf): \n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-03-03%20at%2011.01.44%20AM.png)\n",
        "\n",
        "```\n",
        "{\"_source\": {\"content\": \"From:Dan Levitan\n",
        "To:Grybauskas, Natalie\n",
        "Subject:RE: Groundbreaking\n",
        "Date:Thursday, February 11, 2016 11:51:00 AM\n",
        "The Pizza place Link is fine, the 14\n",
        "th st. one is not.\n",
        " --Dan Levitan\n",
        " BerlinRosen Public Affairs\"}, \"_id\": \"p1938\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVmKpe5hnsxx"
      },
      "source": [
        "## Chopping each page into a list of sentences\n",
        "\n",
        "We have to do this because pages and paragraphs often cover multiple topics, which might confuse the model. And, Universal Sentence Encoder is built to encode sentences... and so it ignores anything after the 128th word in its input.\n",
        "\n",
        "The code below cuts the text into sentences, but groups any two consecutive sentences under 15 words long together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTA-aV4pH-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5734a29-7cd6-4c16-c87c-8c9685fe3f01"
      },
      "source": [
        "!wc docs.jsonl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    84  42564 273251 docs.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LEh77ZORnrrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "349954f6-39f1-4644-f4ce-3d464935d100"
      },
      "source": [
        "# takes about 10 seconds\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from functools import reduce\n",
        "from w3lib.html import remove_tags\n",
        "\n",
        "import syntok.segmenter as segmenter\n",
        "\n",
        "total_docs = 84 # get this with `wc` (only used for progress bar)\n",
        "\n",
        "total_short_paragraphs = 0\n",
        "MAX_SENT_LEN = 50\n",
        "\n",
        "def sentenceify(text):\n",
        "    return [sl for l in [[''.join([t.spacing + t.value for t in s]) for s in p if len(s) < MAX_SENT_LEN] for p in segmenter.analyze(text)] for sl in l if any(map(lambda x: x.isalpha(), sl))]\n",
        "\n",
        "\n",
        "def clean_html(html):\n",
        "    if \"<\" in html and \">\" in html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "            plist = soup.find('plist')\n",
        "            if plist:\n",
        "                plist.decompose() # remove plists because ugh\n",
        "            text = soup.getText()\n",
        "        except:\n",
        "            text = remove_tags(html)\n",
        "        return '. '.join(text.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "    else:\n",
        "        return '. '.join(html.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "\n",
        "# if this sentence is short, then group it with other short sentences (so you get groups of continuous short sentences, broken up by one-element groups of longer sentences)\n",
        "def short_sentence_grouper_bean_factory(target_sentence_length): # in chars\n",
        "    def group_short_sentences(list_of_lists_of_sentences, next_sentence):\n",
        "        if not list_of_lists_of_sentences:\n",
        "            return [[next_sentence]]\n",
        "        if len(next_sentence) < target_sentence_length:\n",
        "           list_of_lists_of_sentences[-1].append(next_sentence)\n",
        "        else:\n",
        "            list_of_lists_of_sentences.append([next_sentence])\n",
        "            list_of_lists_of_sentences.append([])\n",
        "        return list_of_lists_of_sentences\n",
        "    return group_short_sentences\n",
        "\n",
        "\n",
        "def overlap(document_tokens, target_length):\n",
        "    \"\"\" pseudo-paginate a document by creating lists of tokens of length `target-length` that overlap at 50%\n",
        "    return a list of `target_length`-length lists of tokens, overlapping by 50% representing all the tokens in the document \n",
        "    \"\"\"\n",
        "\n",
        "    overlapped = []\n",
        "    cursor = 0\n",
        "    while len(' '.join(document_tokens[cursor:]).split()) >= target_length:\n",
        "      overlapped.append(document_tokens[cursor:cursor+target_length])\n",
        "      cursor += target_length // 2\n",
        "    return overlapped\n",
        "\n",
        "\n",
        "def sentences_to_short_paragraphs(group_of_sentences, target_length, min_shingle_length=10):\n",
        "    \"\"\" outputting overlapping groups of shorter sentences \n",
        "    \n",
        "        group_of_sentences = list of strings, where each string is a sentences\n",
        "        target_length = max length IN WORDS of output sentennces\n",
        "        min_shingle_length = don't have sentences that differ just in the inclusion of a sentence of this size\n",
        "    \"\"\"\n",
        "    if len(group_of_sentences) == 1:\n",
        "        return [' '.join(group_of_sentences[0].split())]\n",
        "    sentences_as_words = [sent.split() for sent in group_of_sentences]\n",
        "    sentences_as_words = [sentence for sentence in sentences_as_words if [len(word) for word in sentence].count(1) < (len(sentence) * 0.5) ]\n",
        "    paragraphs = []\n",
        "    for i, sentence in enumerate(sentences_as_words[:-1]):\n",
        "        if i > 0 and len(sentence) < min_shingle_length  and len(sentences_as_words[i-1]) < min_shingle_length and i % 2 == 0:\n",
        "            continue # skip really short sentences if the previous one is also really short (but not so often that we lose anything )\n",
        "        buff = list(sentence) # just making a copy.\n",
        "        for subsequent_sentence in sentences_as_words[i+1:]:\n",
        "            if len(buff) + len(subsequent_sentence) <= target_length:\n",
        "                buff += subsequent_sentence\n",
        "            else:\n",
        "                break\n",
        "        paragraphs.append(buff)\n",
        "    return [' '.join(graf) for graf in paragraphs]\n",
        "\n",
        "\n",
        "def to_short_paragraphs(text, paragraph_len=15, min_sentence_len=8): # paragraph_len in words, min_sentence_len in chars\n",
        "    sentences = sentenceify( clean_html(text) )\n",
        "    grouped_sentences = reduce(short_sentence_grouper_bean_factory(150) , sentences, [])\n",
        "    return [sl for l in [sentences_to_short_paragraphs(group, paragraph_len) for group in grouped_sentences if len(group) >= 2 or (len(group) > 0 and len(group[0]) > min_sentence_len)] for sl in l if sl]\n",
        "\n",
        "paragraph_target_length = 10\n",
        "\n",
        "with open(f\"docs-sentences{paragraph_target_length}.json\", 'w') as writer: \n",
        "    with open('docs.jsonl', 'r') as reader:\n",
        "        for i, line_json in tqdm(enumerate(reader), total=total_docs):\n",
        "            line = json.loads(line_json)\n",
        "            text = line[\"_source\"][\"content\"][:1000000]\n",
        "            for j, page in enumerate(to_short_paragraphs(text, paragraph_target_length)):\n",
        "                total_short_paragraphs += 1\n",
        "                writer.write(json.dumps({\n",
        "                    \"text\": page, \n",
        "                    \"_id\": line[\"_id\"], \n",
        "                    \"chonk\": j,\n",
        "                    # \"routing\": line.get(\"_routing\", None),\n",
        "                    # \"path\": line[\"_source\"][\"path\"]\n",
        "                    }) + \"\\n\")\n",
        "print(f\"total paragraphs: {total_short_paragraphs}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:00<00:00, 179.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total paragraphs: 1629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqvfzGZVueGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "97ede334-13a6-4fa3-c8a7-2adac20bedca"
      },
      "source": [
        "!head docs-sentences10.json"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"text\": \"I wanted to say something earlier but was afraid it would come out wrong or that I wouldn't be able to find the words to express how I really feel.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 0}\n",
            "{\"text\": \"However, I\\u2019ve realized that staying silent is far worse because, like you;\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 1}\n",
            "{\"text\": \"The murders of George Floyd in Minneapolis, Breonna Taylor in Kentucky, and Ahmaud Arbery in Georgia are the most recent names added to a lengthy list of horrors faced by black people over the past several hundred years.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 2}\n",
            "{\"text\": \"During this troublesome time, even when most people are craving normalcy, we must not turn a blind eye to injustices and continue to stand on the sidelines.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 3}\n",
            "{\"text\": \"Returning to the status quo will only perpetuate the damage being done.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 4}\n",
            "{\"text\": \"Everyone must do more to support the Black community.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 5}\n",
            "{\"text\": \"Taking the time to educate yourself and understand someone else\\u2019s perspective is one of the simplest, yet most powerful ways to show your support.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 6}\n",
            "{\"text\": \"I'll admit until recently I didn't understand what the Black Lives Matter movement was actually about.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 7}\n",
            "{\"text\": \"Why were the \\u201cAll Lives Matter\\u201d social media posts looked at as racist?\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 8}\n",
            "{\"text\": \"I had to be missing something.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeyDaFbxwbD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "79641189-722d-493c-b166-fae2ff893442"
      },
      "source": [
        "!head docs-sentences15.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"text\": \"I wanted to say something earlier but was afraid it would come out wrong or that I wouldn't be able to find the words to express how I really feel.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 0}\n",
            "{\"text\": \"However, I\\u2019ve realized that staying silent is far worse because, like you;\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 1}\n",
            "{\"text\": \"The murders of George Floyd in Minneapolis, Breonna Taylor in Kentucky, and Ahmaud Arbery in Georgia are the most recent names added to a lengthy list of horrors faced by black people over the past several hundred years.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 2}\n",
            "{\"text\": \"During this troublesome time, even when most people are craving normalcy, we must not turn a blind eye to injustices and continue to stand on the sidelines.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 3}\n",
            "{\"text\": \"Returning to the status quo will only perpetuate the damage being done.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 4}\n",
            "{\"text\": \"Everyone must do more to support the Black community.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 5}\n",
            "{\"text\": \"Taking the time to educate yourself and understand someone else\\u2019s perspective is one of the simplest, yet most powerful ways to show your support.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 6}\n",
            "{\"text\": \"I'll admit until recently I didn't understand what the Black Lives Matter movement was actually about.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 7}\n",
            "{\"text\": \"Why were the \\u201cAll Lives Matter\\u201d social media posts looked at as racist?\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 8}\n",
            "{\"text\": \"I had to be missing something. Understanding is just the first step.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxAFAJI9xsAU"
      },
      "source": [
        "# Creating a Multilingual Semantic-Similarity Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3DIT9uT7Z34"
      },
      "source": [
        "## Using a pre-trained model to transform sentences into vectors\n",
        "\n",
        "We compute embeddings in _batches_ so that they fit in the GPU's RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRoRT5qCEIYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf1fd276-7427-4031-8327-97fefed72978"
      },
      "source": [
        "# Takes about 12 seconds\n",
        "vector_index_chunk = AnnoyIndex(512, 'angular')  # Length of item vector that will be indexed\n",
        "\n",
        "batch_size = 256\n",
        "docs = {}\n",
        "\n",
        "doc_counter = 0\n",
        "with tqdm(total=37281) as pbar:\n",
        "  for j, batch in enumerate(pd.read_json('docs-sentences10.json', lines=True, chunksize=batch_size)):\n",
        "    batch_vecs = session.run(embedded_text, feed_dict={text_input: batch[\"text\"]})\n",
        "    # sentences.extend(batch[\"text\"])\n",
        "    pbar.update(len(batch))\n",
        "    doc_idxs = list(range(doc_counter, doc_counter + batch_size))\n",
        "    for vec, page_num, doc in zip(batch_vecs, doc_idxs, batch.iterrows()):\n",
        "      vector_index_chunk.add_item(page_num, vec)\n",
        "      docs[page_num] = doc[1][\"_id\"]\n",
        "    doc_counter += batch_size\n",
        "    \n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1629/37281 [00:08<03:07, 190.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oeBqoE8e-scg"
      },
      "source": [
        "## Building an index of semantic vectors\n",
        "\n",
        "We use the [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SmoB9I9Pe4IT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7839848b-29cc-4cee-d54e-7b4487ac8bed"
      },
      "source": [
        "vector_index_chunk.build(10) # 10 trees"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHcd4w22p3g-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e1b3839-78e1-43ec-a7b4-d6f3de583e20"
      },
      "source": [
        "vector_index_chunk.save('docs_annoy_small.bin') # you could save this and skip the step above, if you'd like"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRVX_CFopDyy"
      },
      "source": [
        "What's indexed in Annoy is a meaningless set of 512 numbers for each sentence. Computers can sort of understand this, but humans can't. So we load up into memory the list of all the sentences, so we can print those as the result.\n",
        "\n",
        "This demo uses a fairly small (5mb) set of documents. If you were using this in \"real life\" you'd probably want to use a database to hold onto these -- they'd be too big to hold in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxzNAzI6mwtH",
        "colab": {}
      },
      "source": [
        "doc_texts = pd.read_json('docs-sentences10.json', lines=True);"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kg9cw0S2_ntQ"
      },
      "source": [
        "## Verify that the semantic-similarity search engine works\n",
        "\n",
        "Let's search for some stuff!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dxu66S8wJIG9"
      },
      "source": [
        "*   Try a few different sample sentences\n",
        "*   Try changing the number of returned results (they are returned in order of similarity)\n",
        "\n",
        "Once you've tried it out a bit, click the menu button to the left, and click Form -> Show Code to see what this is doing under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S0zd96xkY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2dbb37f0-668b-44ba-a4e4-abe03e8e388d"
      },
      "source": [
        "doc_texts"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>_id</th>\n",
              "      <th>chonk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I wanted to say something earlier but was afra...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>However, I’ve realized that staying silent is ...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The murders of George Floyd in Minneapolis, Br...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>During this troublesome time, even when most p...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Returning to the status quo will only perpetua...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>It’s also why today Intel is pledging $1 milli...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1625</th>\n",
              "      <td>I also encourage employees to consider donatin...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1626</th>\n",
              "      <td>It’s with a heavy heart that I write this note...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>I know I speak for the leadership team, our bo...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>Together we will get through this.</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1629 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ... chonk\n",
              "0     I wanted to say something earlier but was afra...  ...     0\n",
              "1     However, I’ve realized that staying silent is ...  ...     1\n",
              "2     The murders of George Floyd in Minneapolis, Br...  ...     2\n",
              "3     During this troublesome time, even when most p...  ...     3\n",
              "4     Returning to the status quo will only perpetua...  ...     4\n",
              "...                                                 ...  ...   ...\n",
              "1624  It’s also why today Intel is pledging $1 milli...  ...    29\n",
              "1625  I also encourage employees to consider donatin...  ...    30\n",
              "1626  It’s with a heavy heart that I write this note...  ...    31\n",
              "1627  I know I speak for the leadership team, our bo...  ...    32\n",
              "1628                 Together we will get through this.  ...    33\n",
              "\n",
              "[1629 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJIJflDrzeob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "194cd897-187b-412b-da50-749685fec5c9"
      },
      "source": [
        "doc_texts.iloc[1628]['text']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Together we will get through this.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "_EFSd65B_mq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "4eae9864-019d-4a72-c7f6-494716b98f5a"
      },
      "source": [
        "sample_query = \"These events impact us, our customers and the communities we serve, and we are called to action. \"  #@param [\"the subway is very crowded now\", \"Some neighborhoods don't have access to healthy fruits and vegetables.\", \"homelessness is up\"] {allow-input: true}\n",
        "num_results = 15  #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
        "\n",
        "search_results = vector_index_chunk.get_nns_by_vector(query_embedding, n=num_results)\n",
        "\n",
        "print('sentences similar to: \"{}\"\\n'.format(sample_query))\n",
        "# search_results\n",
        "\n",
        "for idx, result_idx in enumerate(search_results):\n",
        "  page_num = docs[result_idx]\n",
        "  text = doc_texts.iloc[result_idx][\"text\"]\n",
        "  print(f\"{idx + 1}, \\\"{text}\\\", {page_num}\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences similar to: \"These events impact us, our customers and the communities we serve, and we are called to action. \"\n",
            "\n",
            "1, \"These events impact us, our customers and the communities we serve, and we are called to action.\", Exelon.txt\n",
            "2, \"We will continue our focus on being more representative of our consumers while doing our part in the communities we serve.”\", nike.txt\n",
            "3, \"Thank you for all you are doing to support our customers, our communities, and each other.\", Wells Fargo.txt\n",
            "4, \"We also will continue to support collective actions and pledges across the business community, such as the Business Roundtable, the Atlanta Committee for Progress and CEO Action for Diversity and Inclusion.\", Coca Cola.txt\n",
            "5, \"Beyond our everyday individual behaviors, we will also act collectively as a company.\", Walmart.txt\n",
            "6, \"Further, we are observing Juneteenth as an annual company holiday to provide Allstaters the opportunity to reflect on this monumental event and engage in their communities.\", Allstate.txt\n",
            "7, \"- In light of my call to action and recent events, this Friday, June 19, AbbVie will observe Juneteenth to take the time to reflect on the changes that have to occur in our Company, our communities and society as a whole.\", AbbVie.txt\n",
            "8, \"We act for one another, our customers, our dealers, our communities, and the world around us.\", John Deere.txt\n",
            "9, \"We remain steadfast in our commitment to fairness and equity for our employees, members and communities.\", Cosco.txt\n",
            "10, \"We’re openly listening to our associates and community partners, and we’re engaging advocacy groups to further understand what more we can do.\", Kroger.txt\n",
            "11, \"The actions listed below are not exhaustive but reflect some of the many steps in our efforts to reaffirm and reinforce our core values as well as support our country and our team members as we work to drive real, lasting change for our society.\", Tyson.txt\n",
            "12, \"Coca-Cola is committed to making a difference in our communities and within our company by mobilizing our history of advancing civil rights and by rallying the strength of our employees, families and friends.\", Coca Cola.txt\n",
            "13, \"Our response to COVID-19 is an example of how when we come together, united against a common enemy, we are unstoppable in helping each other, our clients and other stakeholders.\", AIG.txt\n",
            "14, \"Over the past year, we have been thinking a lot about the purpose of our organization and the work we do with our customers and partners.\", Cisco.txt\n",
            "15, \"To do our part to promote healing and inclusion during this troubled time, we are launching our Be The Change program to uplift our workforce and our communities through dialogue, education, and engagement.\", TIAA.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i6YvctORJWgS"
      },
      "source": [
        "## Wait, how did that work?\n",
        "\n",
        "### Nearest neighbors -- it's what it sounds like.\n",
        "\n",
        "When is a sentence \"similar\" to another?\n",
        "\n",
        "Remember those 512-dimensional vectors? We're treating two sentences as similar if their vectors are close together. Our search results are \"nearest neighbors,\" which is what it sounds like.\n",
        "\n",
        "Imagine the vectors were just three dimensions and we had four sentences, encoded as:\n",
        "\n",
        "1. [1, 2, 1]\n",
        "2. [100, 600, -12]\n",
        "3. [5, 7, 3]\n",
        "4. [-50, 1, -5798]\n",
        "\n",
        "Which sentence is probably the most similar to sentence #1?\n",
        "\n",
        "\"Annoy\" is a library that makes this easier to calculate quickly for hundreds of thousands of sentences. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "311mS63qpp84"
      },
      "source": [
        "**Copyright 2019 The TensorFlow Hub Authors and Quartz.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "JMyTNwSJGGWg",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors and Quartz All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
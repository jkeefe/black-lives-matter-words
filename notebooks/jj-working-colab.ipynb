{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mayoral USE searcher for NICAR",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Document Search with Sentence Encoder\n",
        "\n",
        "Originally by by Jeremy Merrill, Quartz.\n",
        "\n",
        "for NICAR 2020.\n",
        "\n",
        "Imagine getting a huge pile of documents. You know that there's interesting stuff in the pile, but you don't know what, exactly. \n",
        "\n",
        "Let's search that pile.\n",
        "\n",
        "Github repo: https://github.com/Quartz/aistudio-searching-data-dumps-with-use / https://github.com/Quartz/aistudio-workshops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63Pd3nJnTl-i"
      },
      "source": [
        "**IMPORTANT** Note: Please select \"**Python 3**\" _and_ \"**GPU**\" in the ***Runtime->Change Runtime type*** dropdown menu above _before_ running this notebook for faster execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This is an interactive demo. You can run all the code necessary to search a pile of documents right here. (A medium-sized, since we don't have all day.)\n",
        "\n",
        "We're using two neat pieces of technology called the *Universal Sentence Encoder* and *Annoy*.\n",
        "\n",
        "- the *Universal Sentence Encoder* is a pre-trained machine-learning model that sorta understands human language. If you feed in a sentence, it comes out with 512 numbers that represent the approximate meaning of that sentence. What's really cool is that if you feed in a second sentence that means about the same thing, that second sentence's numbers will be very close to those of the first sentence.\n",
        "- *Annoy* is a library that makes it really easy to find points in vector space that are close to each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrZqHVFBGfW"
      },
      "source": [
        "What's \"vector space\"? Imagine dot plot with an x-axis and a y-axis. That's two-dimensional vector space.\n",
        "\n",
        "This is three-dimensional vector space. Three axes: x, y, z.\n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-02-28%20at%205.43.59%20PM.png)\n",
        "\n",
        "Now imagine 512 axes. That's what we're dealing with here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZPVY12mDb0P"
      },
      "source": [
        "## Okay, let's get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f453ae9-0214-4815-e85e-2ae271a30c74"
      },
      "source": [
        "#@title Setup Environment\n",
        "#latest Tensorflow that supports sentencepiece is 1.14\n",
        "!pip uninstall --quiet --yes tensorflow\n",
        "!pip install --quiet tensorflow-gpu==1.14\n",
        "!pip install --quiet tensorflow==1.14\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet bokeh\n",
        "!pip install --quiet tf-sentencepiece\n",
        "!pip install --quiet annoy\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet w3lib\n",
        "!pip install --quiet syntok"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████        | 281.9MB 1.3MB/s eta 0:01:16\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/urllib3/response.py\", line 425, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/urllib3/response.py\", line 507, in read\n",
            "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 459, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 503, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 1012, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 631, in read\n",
            "    v = self._sslobj.read(len, buffer)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 382, in run\n",
            "    resolver.resolve(requirement_set)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 201, in resolve\n",
            "    self._resolve_one(requirement_set, req)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 365, in _resolve_one\n",
            "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 313, in _get_abstract_dist_for\n",
            "    req, self.session, self.finder, self.require_hashes\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/prepare.py\", line 194, in prepare_linked_requirement\n",
            "    progress_bar=self.progress_bar\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 465, in unpack_url\n",
            "    progress_bar=progress_bar\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 316, in unpack_http_url\n",
            "    progress_bar)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 551, in _download_http_url\n",
            "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 253, in _download_url\n",
            "    hashes.check_against_chunks(downloaded_chunks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/utils/hashes.py\", line 80, in check_against_chunks\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 223, in written_chunks\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
            "    for x in it:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/download.py\", line 212, in resp_read\n",
            "    decode_content=False):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/urllib3/response.py\", line 564, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/urllib3/response.py\", line 529, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
            "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 31kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 59.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 2.6MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FhAXYYn-ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "44c68911-0e68-4c9f-a836-9e40409b32d3"
      },
      "source": [
        "!pip install tf-sentencepiece"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/a5/16123d662ebeb087552c39c895e9ec6239fb828e236d95fdf67b20907b27/tf_sentencepiece-0.1.90-py2.py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tf-sentencepiece\n",
            "Successfully installed tf-sentencepiece-0.1.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha",
        "colab": {}
      },
      "source": [
        "#@title Setup common imports and functions\n",
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "mkmF3w8WGLcM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "299c5e2b-a6e6-4566-e530-ed64536a63d7"
      },
      "source": [
        "#@title get the machine learning stuff set up. (boilerplate!)\n",
        "# this version of the Universal Sentence Encoder only \"speaks\" English\n",
        "# but there's another version you can switch in that supports 16 different languages!\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
        "\n",
        "# boilerplate, getting started with Tensorflow.\n",
        "# (how to use Tensorflow is way outside the scope of this class)\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  multiling_embed = hub.Module(module_url)\n",
        "  embedded_text = multiling_embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XjNsxF2b7ZU",
        "colab": {}
      },
      "source": [
        "# @ get the data.\n",
        "# let's get our data!\n",
        "# it's a JSONL file, which is a file with one page, as its own JSON document, per line.\n",
        "!wget --quiet -nc -O docs.jsonl https://www.dropbox.com/s/rwe8rrrhuvw0eau/docs.jsonl"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eExMTHZNGn86"
      },
      "source": [
        "## What the heck is JSONL?\n",
        "\n",
        "Don't worry too much about it. It looks like this, but there's nothing special to it, it's just a way to get the content of the pages in [these emails](https://github.com/Quartz/aistudio-doc2vec-for-investigative-journalism/blob/master/2018.05.24_BerlinRosen_Responsive_Records.pdf): \n",
        "\n",
        "![alt text](https://filedn.com/lVaAxkskVxILBoUDG3XUrm7/nicar20presentation/Screen%20Shot%202020-03-03%20at%2011.01.44%20AM.png)\n",
        "\n",
        "```\n",
        "{\"_source\": {\"content\": \"From:Dan Levitan\n",
        "To:Grybauskas, Natalie\n",
        "Subject:RE: Groundbreaking\n",
        "Date:Thursday, February 11, 2016 11:51:00 AM\n",
        "The Pizza place Link is fine, the 14\n",
        "th st. one is not.\n",
        " --Dan Levitan\n",
        " BerlinRosen Public Affairs\"}, \"_id\": \"p1938\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVmKpe5hnsxx"
      },
      "source": [
        "## Chopping each page into a list of sentences\n",
        "\n",
        "We have to do this because pages and paragraphs often cover multiple topics, which might confuse the model. And, Universal Sentence Encoder is built to encode sentences... and so it ignores anything after the 128th word in its input.\n",
        "\n",
        "The code below cuts the text into sentences, but groups any two consecutive sentences under 15 words long together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTA-aV4pH-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b365763-f951-46d5-e9ae-fc70137cc2e2"
      },
      "source": [
        "!wc docs.jsonl"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    83  41874 269064 docs.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LEh77ZORnrrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cffb2239-ecef-426b-990e-fd05aa7c06ca"
      },
      "source": [
        "# takes about 10 seconds\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from functools import reduce\n",
        "from w3lib.html import remove_tags\n",
        "\n",
        "import syntok.segmenter as segmenter\n",
        "\n",
        "total_docs = 83 # get this with `wc` (only used for progress bar)\n",
        "\n",
        "total_short_paragraphs = 0\n",
        "MAX_SENT_LEN = 50\n",
        "\n",
        "def sentenceify(text):\n",
        "    return [sl for l in [[''.join([t.spacing + t.value for t in s]) for s in p if len(s) < MAX_SENT_LEN] for p in segmenter.analyze(text)] for sl in l if any(map(lambda x: x.isalpha(), sl))]\n",
        "\n",
        "\n",
        "def clean_html(html):\n",
        "    if \"<\" in html and \">\" in html:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "            plist = soup.find('plist')\n",
        "            if plist:\n",
        "                plist.decompose() # remove plists because ugh\n",
        "            text = soup.getText()\n",
        "        except:\n",
        "            text = remove_tags(html)\n",
        "        return '. '.join(text.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "    else:\n",
        "        return '. '.join(html.split(\"\\r\\n\\r\\n\\r\\n\"))\n",
        "\n",
        "# if this sentence is short, then group it with other short sentences (so you get groups of continuous short sentences, broken up by one-element groups of longer sentences)\n",
        "def short_sentence_grouper_bean_factory(target_sentence_length): # in chars\n",
        "    def group_short_sentences(list_of_lists_of_sentences, next_sentence):\n",
        "        if not list_of_lists_of_sentences:\n",
        "            return [[next_sentence]]\n",
        "        if len(next_sentence) < target_sentence_length:\n",
        "           list_of_lists_of_sentences[-1].append(next_sentence)\n",
        "        else:\n",
        "            list_of_lists_of_sentences.append([next_sentence])\n",
        "            list_of_lists_of_sentences.append([])\n",
        "        return list_of_lists_of_sentences\n",
        "    return group_short_sentences\n",
        "\n",
        "\n",
        "def overlap(document_tokens, target_length):\n",
        "    \"\"\" pseudo-paginate a document by creating lists of tokens of length `target-length` that overlap at 50%\n",
        "    return a list of `target_length`-length lists of tokens, overlapping by 50% representing all the tokens in the document \n",
        "    \"\"\"\n",
        "\n",
        "    overlapped = []\n",
        "    cursor = 0\n",
        "    while len(' '.join(document_tokens[cursor:]).split()) >= target_length:\n",
        "      overlapped.append(document_tokens[cursor:cursor+target_length])\n",
        "      cursor += target_length // 2\n",
        "    return overlapped\n",
        "\n",
        "\n",
        "def sentences_to_short_paragraphs(group_of_sentences, target_length, min_shingle_length=10):\n",
        "    \"\"\" outputting overlapping groups of shorter sentences \n",
        "    \n",
        "        group_of_sentences = list of strings, where each string is a sentences\n",
        "        target_length = max length IN WORDS of output sentennces\n",
        "        min_shingle_length = don't have sentences that differ just in the inclusion of a sentence of this size\n",
        "    \"\"\"\n",
        "    if len(group_of_sentences) == 1:\n",
        "        return [' '.join(group_of_sentences[0].split())]\n",
        "    sentences_as_words = [sent.split() for sent in group_of_sentences]\n",
        "    sentences_as_words = [sentence for sentence in sentences_as_words if [len(word) for word in sentence].count(1) < (len(sentence) * 0.5) ]\n",
        "    paragraphs = []\n",
        "    for i, sentence in enumerate(sentences_as_words[:-1]):\n",
        "        if i > 0 and len(sentence) < min_shingle_length  and len(sentences_as_words[i-1]) < min_shingle_length and i % 2 == 0:\n",
        "            continue # skip really short sentences if the previous one is also really short (but not so often that we lose anything )\n",
        "        buff = list(sentence) # just making a copy.\n",
        "        for subsequent_sentence in sentences_as_words[i+1:]:\n",
        "            if len(buff) + len(subsequent_sentence) <= target_length:\n",
        "                buff += subsequent_sentence\n",
        "            else:\n",
        "                break\n",
        "        paragraphs.append(buff)\n",
        "    return [' '.join(graf) for graf in paragraphs]\n",
        "\n",
        "\n",
        "def to_short_paragraphs(text, paragraph_len=15, min_sentence_len=8): # paragraph_len in words, min_sentence_len in chars\n",
        "    sentences = sentenceify( clean_html(text) )\n",
        "    grouped_sentences = reduce(short_sentence_grouper_bean_factory(150) , sentences, [])\n",
        "    return [sl for l in [sentences_to_short_paragraphs(group, paragraph_len) for group in grouped_sentences if len(group) >= 2 or (len(group) > 0 and len(group[0]) > min_sentence_len)] for sl in l if sl]\n",
        "\n",
        "paragraph_target_length = 5\n",
        "\n",
        "with open(f\"docs-sentences{paragraph_target_length}.json\", 'w') as writer: \n",
        "    with open('docs.jsonl', 'r') as reader:\n",
        "        for i, line_json in tqdm(enumerate(reader), total=total_docs):\n",
        "            line = json.loads(line_json)\n",
        "            text = line[\"_source\"][\"content\"][:1000000]\n",
        "            for j, page in enumerate(to_short_paragraphs(text, paragraph_target_length)):\n",
        "                total_short_paragraphs += 1\n",
        "                writer.write(json.dumps({\n",
        "                    \"text\": page, \n",
        "                    \"_id\": line[\"_id\"], \n",
        "                    \"chonk\": j,\n",
        "                    # \"routing\": line.get(\"_routing\", None),\n",
        "                    # \"path\": line[\"_source\"][\"path\"]\n",
        "                    }) + \"\\n\")\n",
        "print(f\"total paragraphs: {total_short_paragraphs}\")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83/83 [00:00<00:00, 174.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total paragraphs: 1601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqvfzGZVueGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "773f9cab-10ce-4a34-b1a1-151c185dd8fc"
      },
      "source": [
        "!head docs-sentences10.json"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"text\": \"I wanted to say something earlier but was afraid it would come out wrong or that I wouldn't be able to find the words to express how I really feel.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 0}\n",
            "{\"text\": \"However, I\\u2019ve realized that staying silent is far worse because, like you;\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 1}\n",
            "{\"text\": \"The murders of George Floyd in Minneapolis, Breonna Taylor in Kentucky, and Ahmaud Arbery in Georgia are the most recent names added to a lengthy list of horrors faced by black people over the past several hundred years.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 2}\n",
            "{\"text\": \"During this troublesome time, even when most people are craving normalcy, we must not turn a blind eye to injustices and continue to stand on the sidelines.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 3}\n",
            "{\"text\": \"Returning to the status quo will only perpetuate the damage being done.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 4}\n",
            "{\"text\": \"Everyone must do more to support the Black community.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 5}\n",
            "{\"text\": \"Taking the time to educate yourself and understand someone else\\u2019s perspective is one of the simplest, yet most powerful ways to show your support.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 6}\n",
            "{\"text\": \"I'll admit until recently I didn't understand what the Black Lives Matter movement was actually about.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 7}\n",
            "{\"text\": \"Why were the \\u201cAll Lives Matter\\u201d social media posts looked at as racist?\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 8}\n",
            "{\"text\": \"I had to be missing something.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeyDaFbxwbD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "79641189-722d-493c-b166-fae2ff893442"
      },
      "source": [
        "!head docs-sentences15.json"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"text\": \"I wanted to say something earlier but was afraid it would come out wrong or that I wouldn't be able to find the words to express how I really feel.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 0}\n",
            "{\"text\": \"However, I\\u2019ve realized that staying silent is far worse because, like you;\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 1}\n",
            "{\"text\": \"The murders of George Floyd in Minneapolis, Breonna Taylor in Kentucky, and Ahmaud Arbery in Georgia are the most recent names added to a lengthy list of horrors faced by black people over the past several hundred years.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 2}\n",
            "{\"text\": \"During this troublesome time, even when most people are craving normalcy, we must not turn a blind eye to injustices and continue to stand on the sidelines.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 3}\n",
            "{\"text\": \"Returning to the status quo will only perpetuate the damage being done.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 4}\n",
            "{\"text\": \"Everyone must do more to support the Black community.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 5}\n",
            "{\"text\": \"Taking the time to educate yourself and understand someone else\\u2019s perspective is one of the simplest, yet most powerful ways to show your support.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 6}\n",
            "{\"text\": \"I'll admit until recently I didn't understand what the Black Lives Matter movement was actually about.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 7}\n",
            "{\"text\": \"Why were the \\u201cAll Lives Matter\\u201d social media posts looked at as racist?\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 8}\n",
            "{\"text\": \"I had to be missing something. Understanding is just the first step.\", \"_id\": \"Berkshire Hathaway.txt\", \"chonk\": 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxAFAJI9xsAU"
      },
      "source": [
        "# Creating a Multilingual Semantic-Similarity Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3DIT9uT7Z34"
      },
      "source": [
        "## Using a pre-trained model to transform sentences into vectors\n",
        "\n",
        "We compute embeddings in _batches_ so that they fit in the GPU's RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRoRT5qCEIYy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b05d5bd4-cef8-44f3-cf02-5d091a6c2886"
      },
      "source": [
        "# Takes about 12 seconds\n",
        "vector_index_chunk = AnnoyIndex(512, 'angular')  # Length of item vector that will be indexed\n",
        "\n",
        "batch_size = 256\n",
        "docs = {}\n",
        "\n",
        "doc_counter = 0\n",
        "with tqdm(total=37281) as pbar:\n",
        "  for j, batch in enumerate(pd.read_json('docs-sentences10.json', lines=True, chunksize=batch_size)):\n",
        "    batch_vecs = session.run(embedded_text, feed_dict={text_input: batch[\"text\"]})\n",
        "    # sentences.extend(batch[\"text\"])\n",
        "    pbar.update(len(batch))\n",
        "    doc_idxs = list(range(doc_counter, doc_counter + batch_size))\n",
        "    for vec, page_num, doc in zip(batch_vecs, doc_idxs, batch.iterrows()):\n",
        "      vector_index_chunk.add_item(page_num, vec)\n",
        "      docs[page_num] = doc[1][\"_id\"]\n",
        "    doc_counter += batch_size\n",
        "    \n",
        "    "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1601/37281 [00:00<00:10, 3523.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oeBqoE8e-scg"
      },
      "source": [
        "## Building an index of semantic vectors\n",
        "\n",
        "We use the [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SmoB9I9Pe4IT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a557a12-c5ed-4d2f-cee2-b7835b20ad89"
      },
      "source": [
        "vector_index_chunk.build(10) # 10 trees"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHcd4w22p3g-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41f36c5d-d886-4b05-82aa-8e4d1d35a3f2"
      },
      "source": [
        "vector_index_chunk.save('docs_annoy_small.bin') # you could save this and skip the step above, if you'd like"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRVX_CFopDyy"
      },
      "source": [
        "What's indexed in Annoy is a meaningless set of 512 numbers for each sentence. Computers can sort of understand this, but humans can't. So we load up into memory the list of all the sentences, so we can print those as the result.\n",
        "\n",
        "This demo uses a fairly small (5mb) set of documents. If you were using this in \"real life\" you'd probably want to use a database to hold onto these -- they'd be too big to hold in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxzNAzI6mwtH",
        "colab": {}
      },
      "source": [
        "doc_texts = pd.read_json('docs-sentences10.json', lines=True);"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kg9cw0S2_ntQ"
      },
      "source": [
        "## Verify that the semantic-similarity search engine works\n",
        "\n",
        "Let's search for some stuff!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dxu66S8wJIG9"
      },
      "source": [
        "*   Try a few different sample sentences\n",
        "*   Try changing the number of returned results (they are returned in order of similarity)\n",
        "\n",
        "Once you've tried it out a bit, click the menu button to the left, and click Form -> Show Code to see what this is doing under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S0zd96xkY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e064bbe9-3062-4ad9-bf0c-850462ac5b04"
      },
      "source": [
        "doc_texts"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>_id</th>\n",
              "      <th>chonk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I wanted to say something earlier but was afra...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>However, I’ve realized that staying silent is ...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The murders of George Floyd in Minneapolis, Br...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>During this troublesome time, even when most p...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Returning to the status quo will only perpetua...</td>\n",
              "      <td>Berkshire Hathaway.txt</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>It’s also why today Intel is pledging $1 milli...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>I also encourage employees to consider donatin...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>It’s with a heavy heart that I write this note...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>I know I speak for the leadership team, our bo...</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>Together we will get through this.</td>\n",
              "      <td>Intel.txt</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1601 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ... chonk\n",
              "0     I wanted to say something earlier but was afra...  ...     0\n",
              "1     However, I’ve realized that staying silent is ...  ...     1\n",
              "2     The murders of George Floyd in Minneapolis, Br...  ...     2\n",
              "3     During this troublesome time, even when most p...  ...     3\n",
              "4     Returning to the status quo will only perpetua...  ...     4\n",
              "...                                                 ...  ...   ...\n",
              "1596  It’s also why today Intel is pledging $1 milli...  ...    29\n",
              "1597  I also encourage employees to consider donatin...  ...    30\n",
              "1598  It’s with a heavy heart that I write this note...  ...    31\n",
              "1599  I know I speak for the leadership team, our bo...  ...    32\n",
              "1600                 Together we will get through this.  ...    33\n",
              "\n",
              "[1601 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJIJflDrzeob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "130ed2ac-513c-44f9-849c-ff8f8f1a9e56"
      },
      "source": [
        "doc_texts.iloc[1600]['text']"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Together we will get through this.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "_EFSd65B_mq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "51e3e6d9-7858-491d-d989-91b60d7b3f83"
      },
      "source": [
        "sample_query = 'I encourage you to consider donating'  #@param [\"the subway is very crowded now\", \"Some neighborhoods don't have access to healthy fruits and vegetables.\", \"homelessness is up\"] {allow-input: true}\n",
        "num_results = 10  #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
        "\n",
        "search_results = vector_index_chunk.get_nns_by_vector(query_embedding, n=num_results)\n",
        "\n",
        "print('sentences similar to: \"{}\"\\n'.format(sample_query))\n",
        "# search_results\n",
        "\n",
        "for idx, result_idx in enumerate(search_results):\n",
        "  page_num = docs[result_idx]\n",
        "  text = doc_texts.iloc[result_idx][\"text\"]\n",
        "  print(f\"Index:{idx}, result_idx:{result_idx} -- {page_num}: {text}\")\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences similar to: \"I encourage you to consider donating\"\n",
            "\n",
            "Index:0, result_idx:1071 -- ALphabet (Google).txt: • As a result of last week’s internal giving campaign, I‘m pleased to share that you all have contributed an additional $2.5 million in donations that we’re matching.\n",
            "Index:1, result_idx:1382 -- AbbVie.txt: I know I can count on you to help us make a positive impact.\n",
            "Index:2, result_idx:799 -- Progressive.txt: We must and will do more.\n",
            "Index:3, result_idx:693 -- Tyson.txt: ▪ We have a track record of charitable giving in the communities where we live and work.\n",
            "Index:4, result_idx:837 -- Kroger.txt: We’ll take action and share our progress.\n",
            "Index:5, result_idx:132 -- Goldman Sachs.txt: For my part, I continue to stand with all of you in supporting our broad COVID-19 relief efforts which are helping to improve the lives of so many around the world.\n",
            "Index:6, result_idx:12 -- Berkshire Hathaway.txt: Each of us can make a difference through our actions, donations, and vote.\n",
            "Index:7, result_idx:648 -- Cisco.txt: Each of us knows the familiar and comfortable playbook of donating funds and releasing a statement of solidarity.\n",
            "Index:8, result_idx:1146 -- Facebook.txt: I encourage you all to also check out Maxine’s post about how you can give direct feedback on product, integrity and content policy ideas as well.\n",
            "Index:9, result_idx:31 -- Honeywell.txt: We will support CYC not only with funding but also with the time and attention of our leadership volunteers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i6YvctORJWgS"
      },
      "source": [
        "## Wait, how did that work?\n",
        "\n",
        "### Nearest neighbors -- it's what it sounds like.\n",
        "\n",
        "When is a sentence \"similar\" to another?\n",
        "\n",
        "Remember those 512-dimensional vectors? We're treating two sentences as similar if their vectors are close together. Our search results are \"nearest neighbors,\" which is what it sounds like.\n",
        "\n",
        "Imagine the vectors were just three dimensions and we had four sentences, encoded as:\n",
        "\n",
        "1. [1, 2, 1]\n",
        "2. [100, 600, -12]\n",
        "3. [5, 7, 3]\n",
        "4. [-50, 1, -5798]\n",
        "\n",
        "Which sentence is probably the most similar to sentence #1?\n",
        "\n",
        "\"Annoy\" is a library that makes this easier to calculate quickly for hundreds of thousands of sentences. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "311mS63qpp84"
      },
      "source": [
        "**Copyright 2019 The TensorFlow Hub Authors and Quartz.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "JMyTNwSJGGWg",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors and Quartz All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}